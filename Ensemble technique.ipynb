{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   **Theorictical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ***Can we use Bagging for regression problems?***\n",
    "*Answer-*\n",
    "\n",
    "Yes, **Bagging (Bootstrap Aggregating)** can definitely be used for **regression problems**, not just for classification tasks. In fact, **Bagging** is widely applied in regression tasks to **reduce variance**, improve model stability, and **enhance prediction accuracy**. \n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **What is Bagging in Machine Learning?**\n",
    "**Bagging (Bootstrap Aggregating)** is an **ensemble learning technique** that combines the predictions of multiple base models (weak learners) to produce a **more accurate and stable prediction**. \n",
    "\n",
    "The core idea is:  \n",
    "- **Randomly sample subsets of data (with replacement)** from the original dataset.\n",
    "- **Train a base model (weak learner)** on each subset.\n",
    "- **Aggregate the predictions** from each model (by averaging in regression or voting in classification).\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Why Use Bagging for Regression Problems?**\n",
    "In **regression tasks**, the target variable is continuous (numeric). The **prediction aggregation** in Bagging for regression is done using:\n",
    "\n",
    "#### **Prediction Aggregation in Regression**\n",
    "- **Averaging the predictions** from multiple models.  \n",
    "- This reduces **variance** and makes the model more **robust**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **How Bagging Works in Regression**\n",
    "#### **Step 1: Bootstrap Sampling (With Replacement)**\n",
    "- Suppose we have a dataset of **1000 samples**.\n",
    "- Bagging will randomly sample **1000 samples with replacement** to create a new subset (some samples may appear more than once, some may not appear).\n",
    "- Repeat this process **K times** (K = number of base models).\n",
    "\n",
    "#### **Step 2: Train Base Learners (Weak Learners)**\n",
    "- Train a **base model** (like Decision Tree, Linear Regression, etc.) on each subset.\n",
    "- Each model learns slightly different patterns.\n",
    "\n",
    "#### **Step 3: Aggregate Predictions**\n",
    "- For **regression problems**, Bagging takes the **average prediction** from all base models:\n",
    "\n",
    "ùë¶^=1/ùêæ ùêæ‚àë/ùëñ=1/ùë¶^ùëñ\n",
    "\n",
    "where:  \n",
    "- \\(K\\) = number of models (weak learners)  \n",
    "- \\( \\ {y^}_i \\) = prediction from the \\(i\\)th model. \n",
    "\n",
    "This **averaging effect** reduces the **variance** of the model.\n",
    "\n",
    "#### ‚úÖ **Why Does Bagging Work Well in Regression?**\n",
    "In regression problems, **variance** is a major issue.  \n",
    "- If we use a model like a **Decision Tree**, it tends to **overfit** the training data (high variance).  \n",
    "- By using **Bagging**, the variance is averaged out, resulting in a more **stable and accurate** model.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Which Models Can Be Used for Bagging in Regression?**\n",
    "We can use **any regression model** as a base learner. Common ones are:\n",
    "\n",
    "| Base Model        | Why Use It with Bagging?                                                                                   |\n",
    "|-------------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| ‚úÖ **Decision Tree Regressor** | Works exceptionally well with Bagging (like in Random Forest). Helps reduce overfitting.      |\n",
    "| ‚úÖ **Linear Regression**      | Can reduce the effect of high variance in linear regression models.                             |\n",
    "| ‚úÖ **KNN Regressor**         | Bagging stabilizes the prediction in noisy datasets.                                              |\n",
    "| ‚úÖ **SVR (Support Vector Regressor)** | Can reduce the variance when SVR overfits the data.                                      |\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Advantages of Using Bagging in Regression**\n",
    "| Advantage  | Description |\n",
    "|------------|-------------|\n",
    "| ‚úÖ **Reduces Variance** | Bagging averages out the errors of individual models, reducing overfitting. |\n",
    "| ‚úÖ **Improves Stability** | The final model is more stable and less prone to fluctuations in data. |\n",
    "| ‚úÖ **Handles Noise Better** | Bagging reduces the impact of noisy data by training on random subsets. |\n",
    "| ‚úÖ **Simple to Implement** | Easy to implement using libraries like Scikit-learn. |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **When Should we Use Bagging for Regression?**\n",
    "#### **Bagging is beneficial in regression when:**\n",
    "1. **The base model has high variance** (like **Decision Trees**).\n",
    "2. We have **small or moderate-sized datasets** prone to overfitting.\n",
    "3. We want **stable predictions** without huge fluctuations.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Bonus: Random Forest is a Special Case of Bagging**\n",
    "- **Random Forest** is simply **Bagging + Random Feature Selection**.  \n",
    "- In **Random Forest**, each Decision Tree not only uses a different subset of data but also a different subset of features.  \n",
    "- This further reduces correlation between models, improving accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Comparison: Bagging vs Boosting in Regression**\n",
    "| Feature    | Bagging                        | Boosting                        |\n",
    "|------------|-------------------------------|-----------------------------------|\n",
    "| ‚úÖ **Goal** | Reduce Variance                | Reduce Bias                       |\n",
    "| ‚úÖ **Base Models** | Independently trained    | Sequentially trained              |\n",
    "| ‚úÖ **Aggregation** | Average of predictions    | Weighted sum of predictions        |\n",
    "| ‚úÖ **Overfitting** | Reduces Overfitting       | May lead to Overfitting            |\n",
    "| ‚úÖ **Example** | Random Forest                | Gradient Boosting, XGBoost        |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Summary**\n",
    "| Feature                  | Bagging for Regression                               |\n",
    "|---------------------------|-----------------------------------------------------|\n",
    "| **Purpose**                | Reduce variance and stabilize predictions.         |\n",
    "| **Base Learner**           | Can be any regression model (Decision Tree, etc.).  |\n",
    "| **Aggregation Method**     | Takes the **average** of all predictions.           |\n",
    "| **Reduces Overfitting?**   | ‚úÖ Yes, especially for high variance models.        |\n",
    "| **Scikit-learn Class**     | `BaggingRegressor()`                               |\n",
    "| **Best Use Case**          | When you have high variance models like Decision Trees. |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ üí° Key Insight:\n",
    "üëâ **If the base regression model (like Decision Tree) is prone to overfitting**, then **Bagging** will significantly reduce **variance** and give you **more stable predictions**. üöÄ\n",
    "\n",
    "üëâ For most **regression problems**, using **Bagging Regressor with Decision Tree** is a powerful solution.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 578.2829479784864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=20)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a Bagging Regressor (with multiple Decision Trees)\n",
    "model = BaggingRegressor(estimator=DecisionTreeRegressor(), \n",
    "                         n_estimators=50,  # 50 models\n",
    "                         random_state=42)\n",
    "\n",
    "# Train the ensemble model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate MSE\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ***What is the difference between multiple model training and single model training?***\n",
    "*Answer-*\n",
    "\n",
    "The key difference between **multiple model training** and **single model training** lies in how many models are trained and how predictions are made.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Single Model Training:**\n",
    "**We train only ONE model** on the dataset and use it for predictions.\n",
    "\n",
    "### ‚úÖ **How it Works**\n",
    "1. **Train a single model** (e.g., Linear Regression, Decision Tree, or Neural Network).\n",
    "2. **Evaluate the model** and check its performance (accuracy, MSE, etc.).\n",
    "3. **Deploy the model** for predictions.\n",
    "\n",
    "### ‚úÖ **Example: Single Decision Tree for House Price Prediction**\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=20)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a single Decision Tree model\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "#### ‚úÖ **Advantages of Single Model Training:**\n",
    "‚úî **Simple and fast** (only one model to train).  \n",
    "‚úî **Easy to interpret** (especially for Linear Regression, Decision Trees).  \n",
    "‚úî **Good for small datasets** where ensemble methods are unnecessary.  \n",
    "\n",
    "### ‚ùå **Disadvantages of Single Model Training:**\n",
    "‚ùå **Can overfit** (especially with high variance models like Decision Trees).  \n",
    "‚ùå **Less robust** (one model‚Äôs mistake affects all predictions).  \n",
    "‚ùå **Lower accuracy** compared to ensembles.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Multiple Model Training (Ensemble Learning):**\n",
    "**üëâ We train MULTIPLE models** and combine their predictions for a better result.\n",
    "\n",
    "#### ‚úÖ **How it Works**\n",
    "1. **Train multiple models** (either the same type or different models).\n",
    "2. **Combine predictions** (by averaging, voting, or stacking).\n",
    "3. **Final prediction is more stable and accurate**.\n",
    "\n",
    "#### ‚úÖ **Example: Using Bagging (Multiple Decision Trees)**\n",
    "```python\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Train a Bagging Regressor (multiple Decision Trees)\n",
    "model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n",
    "                         n_estimators=50,  # 50 models\n",
    "                         random_state=42)\n",
    "\n",
    "# Train ensemble model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "#### ‚úÖ **Advantages of Multiple Model Training:**\n",
    "‚úî **Higher accuracy** (reduces variance and bias).  \n",
    "‚úî **More stable** (less affected by random errors).  \n",
    "‚úî **Handles complex data better** than a single model.  \n",
    "\n",
    "#### ‚ùå **Disadvantages of Multiple Model Training:**\n",
    "‚ùå **Slower to train** (more models mean more computation).  \n",
    "‚ùå **Harder to interpret** (difficult to understand which model contributes most).  \n",
    "‚ùå **Uses more memory** than a single model.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Comparison: Single Model vs Multiple Model Training**\n",
    "| Feature | **Single Model Training** | **Multiple Model Training** |\n",
    "|------------|---------------------------|-------------------------------|\n",
    "| **Number of Models** | One model | Many models (ensemble) |\n",
    "| **Training Speed** | Faster | Slower (more computations) |\n",
    "| **Accuracy** | Lower | Higher |\n",
    "| **Overfitting Risk** | Higher | Lower (Bagging) |\n",
    "| **Interpretability** | Easier | Harder |\n",
    "| **Use Case** | Small datasets, simple tasks | Complex data, high-accuracy tasks |\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ When to Use Each Approach?**\n",
    "‚úÖ **Use Single Model Training When:**\n",
    "- You need quick results.\n",
    "- Your dataset is small.\n",
    "- Interpretability is important.\n",
    "\n",
    "‚úÖ **Use Multiple Model Training When:**\n",
    "- we need high accuracy.\n",
    "- our data is complex and noisy.\n",
    "- we want a more stable model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Verdict**\n",
    "If we want **quick, simple predictions** ‚Üí **Single Model Training** ‚úÖ  \n",
    "If we want **higher accuracy and stability** ‚Üí **Multiple Model Training (Ensemble)** ‚úÖ  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ***Explain the concept of feature randomness in Random Forest.***\n",
    "*Answer-*\n",
    "\n",
    "#### üå≤ **Feature Randomness in Random Forest-** üå≤  \n",
    "\n",
    "Feature randomness in **Random Forest** refers to the way the algorithm **randomly selects a subset of features** (columns) at each split while building individual decision trees. This process helps to **reduce correlation** between trees, making the model more robust and accurate.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç How Does Feature Randomness Work?**\n",
    "#### **1Ô∏è‚É£ In Standard Decision Trees (No Randomness):**  \n",
    "- At each split, the algorithm **considers all features** and picks the **best one** (based on Gini impurity, entropy, or variance).  \n",
    "- This often leads to **overfitting** because all trees in the forest would look very similar.\n",
    "\n",
    "#### **2Ô∏è‚É£ In Random Forest (Feature Randomness Applied):**  \n",
    "- Instead of considering **all features**, each tree only considers a **random subset** of features at each split.\n",
    "- This introduces **diversity among trees**, reducing overfitting and improving generalization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Key Benefits of Feature Randomness:**\n",
    "‚úÖ **Reduces Overfitting** ‚Äì Since trees are trained on different feature subsets, they don‚Äôt all follow the same patterns.  \n",
    "‚úÖ **Increases Model Diversity** ‚Äì More variation among trees makes the overall forest stronger.  \n",
    "‚úÖ **Improves Performance** ‚Äì Helps in high-dimensional datasets where some features may dominate.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üî¢ How Many Features are Selected at Each Split?**\n",
    "By default, Random Forest selects **‚àö(total features) for classification** and **(total features) / 3 for regression** at each split.\n",
    "\n",
    "#### **Example:**\n",
    "- If there are **100 features** in the dataset:\n",
    "  - üìå **For classification:** Each tree will consider **‚àö100 = 10 features** at each split.\n",
    "  - üìå **For regression:** Each tree will consider **100/3 ‚âà 33 features** at each split.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üåü Summary-**\n",
    "| Feature Randomness in Random Forest | Benefits |\n",
    "|--------------------------------|------------|\n",
    "| ‚úÖ **Random feature selection at each split** | Increases model diversity |\n",
    "| ‚úÖ **Reduces correlation between trees** | Prevents overfitting |\n",
    "| ‚úÖ **Improves generalization** | Works well on high-dimensional data |\n",
    "| ‚úÖ **Helps handle irrelevant features** | Avoids feature dominance |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.1292683  0.01582194 0.4447404  0.41016935]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train Random Forest with Feature Randomness\n",
    "rf = RandomForestClassifier(n_estimators=10, max_features=\"sqrt\", random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Check feature importances\n",
    "print(\"Feature Importances:\", rf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, max_features=\"sqrt\" applies feature randomness, ensuring that each tree considers only a subset of features at each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ***What is OOB (Out-of-Bag) Score?***\n",
    "*Answer-*\n",
    "\n",
    "The **`Out-of-Bag (OOB) Score`** is an internal method used in **`Random Forest`** to measure model accuracy **`without needing a separate validation set`**. It works by using **bootstrap sampling**, where each decision tree is trained on a **random subset** of the data, and the remaining **unused (out-of-bag) data** is used for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç How Does OOB Scoring Work?**\n",
    "1Ô∏è‚É£ **Bootstrap Sampling:** üé≤  \n",
    "   - Each tree in the Random Forest is trained on a **random subset** (‚âà63% of the data).  \n",
    "   - The remaining **‚âà37% of the data** is **not used** for training that tree (this is the \"out-of-bag\" data).\n",
    "\n",
    "2Ô∏è‚É£ **Prediction on OOB Data:** üîç  \n",
    "   - The trained tree is then tested on its **out-of-bag samples**.  \n",
    "   - This process repeats for every tree.\n",
    "\n",
    "3Ô∏è‚É£ **Final OOB Score Calculation:** üèÜ  \n",
    "   - For each sample in the dataset, predictions from **all trees that did not use it for training** are averaged (for regression) or voted (for classification).  \n",
    "   - The final OOB score is the overall accuracy or R¬≤ score.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Why is OOB Score Useful?**\n",
    "‚úÖ **No Need for a Separate Validation Set** ‚Äì Saves data for actual training.  \n",
    "‚úÖ **Provides an Unbiased Estimate of Accuracy** ‚Äì Similar to cross-validation.  \n",
    "‚úÖ **Faster Model Evaluation** ‚Äì Since it's built-in, we don‚Äôt need extra validation steps.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìä OOB Score vs. Test Set Accuracy**\n",
    "| Feature               | OOB Score üèÜ | Test Set Accuracy üéØ |\n",
    "|-----------------------|-------------|----------------------|\n",
    "| ‚úÖ **Purpose**        | Internal validation | External validation |\n",
    "| ‚úÖ **Data Used**     | Unused bootstrap samples | Separate test set |\n",
    "| ‚úÖ **Bias**          | Slightly optimistic | True generalization error |\n",
    "| ‚úÖ **Need Extra Data?** | ‚ùå No | ‚úÖ Yes |\n",
    "\n",
    "üëâ **If OOB Score is close to Test Accuracy**, the model is stable and generalizing well.  \n",
    "üëâ **If OOB Score is much higher**, the model may be **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üåü Summary-**\n",
    "- ‚úÖ **OOB Score** is a built-in cross-validation method for Random Forest.  \n",
    "- ‚úÖ Uses **out-of-bag samples** to measure accuracy **without a validation set**.  \n",
    "- ‚úÖ Helps estimate performance **efficiently** and detect **overfitting**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train Random Forest with OOB Score Enabled\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get OOB Score\n",
    "print(\"OOB Score:\", rf.oob_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ***How can you measure the importance of features in a Random Forest model?***\n",
    "*Answer-*\n",
    "\n",
    "Feature importance in **`Random Forest`** helps us understand which features have the most influence on predictions. This is **`useful for feature selection, interpretability, and improving model performance.`**\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç Methods to Measure Feature Importance:**\n",
    "#### **1Ô∏è‚É£ Mean Decrease in Impurity (MDI) ‚Äì Default Method:**\n",
    "üìå **Concept:**  \n",
    "- Every time a feature is used in a tree split, it reduces impurity (e.g., Gini impurity for classification or variance for regression).  \n",
    "- The average decrease across all trees gives the feature‚Äôs importance.  \n",
    "\n",
    "üìå **Pros & Cons:**  \n",
    "‚úÖ Fast and easy to compute.  \n",
    "‚úÖ Gives insight into feature importance.  \n",
    "‚ùå Biased toward features with more unique values (continuous variables).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Permutation Importance ‚Äì More Reliable**  \n",
    "üìå **Concept:**  \n",
    "- Shuffle each feature **randomly** and check how much model accuracy drops.  \n",
    "- A larger accuracy drop means the feature is **more important**.  \n",
    "\n",
    "\n",
    "üìå **Pros & Cons:**  \n",
    "‚úÖ More reliable than MDI.  \n",
    "‚úÖ Works with different types of models.  \n",
    "‚ùå Slower, as it requires multiple runs.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ SHAP Values ‚Äì Explainable AI Approach**  \n",
    "üìå **Concept:**  \n",
    "- SHAP (SHapley Additive exPlanations) assigns each feature a contribution score based on its impact on predictions.  \n",
    "- Provides **local and global** feature importance.  \n",
    "\n",
    "\n",
    "üìå **Pros & Cons:**  \n",
    "‚úÖ Most accurate interpretation.  \n",
    "‚úÖ Works for individual predictions.  \n",
    "‚ùå Computationally expensive.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìä Comparing Feature Importance Methods**\n",
    "| Method | Strengths | Weaknesses |\n",
    "|--------|----------|------------|\n",
    "| ‚úÖ **MDI (Default)** | Fast, built-in for Random Forest | Biased toward high-cardinality features |\n",
    "| ‚úÖ **Permutation Importance** | More reliable, works with any model | Computationally expensive |\n",
    "| ‚úÖ **SHAP Values** | Most interpretable, works for individual predictions | Slow, complex |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ When to Use Each Method?**\n",
    "üîπ **Quick insight?** ‚Üí **Use MDI (Default Feature Importance)**  \n",
    "üîπ **More reliable ranking?** ‚Üí **Use Permutation Importance**  \n",
    "üîπ **Deep interpretability?** ‚Üí **Use SHAP Values**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Importance\n",
      "2  petal length (cm)    0.436130\n",
      "3   petal width (cm)    0.436065\n",
      "0  sepal length (cm)    0.106128\n",
      "1   sepal width (cm)    0.021678\n"
     ]
    }
   ],
   "source": [
    "#1 example\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Display as a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Importance\n",
      "2  petal length (cm)    0.222667\n",
      "3   petal width (cm)    0.180667\n",
      "0  sepal length (cm)    0.014667\n",
      "1   sepal width (cm)    0.012667\n"
     ]
    }
   ],
   "source": [
    "#2 example\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute Permutation Importance\n",
    "perm_importance = permutation_importance(rf, X, y, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "\n",
    "# Display Results\n",
    "perm_df = pd.DataFrame({'Feature': feature_names, 'Importance': perm_importance.importances_mean})\n",
    "perm_df = perm_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(perm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ***Explain the working principle of a Bagging Classifier.***\n",
    "*Answer-*\n",
    "\n",
    "###  **Working Principle of a Bagging Classifier-** \n",
    "\n",
    "A **`Bagging Classifier`** (Bootstrap Aggregating Classifier) is an **`ensemble learning technique`** that `improves accuracy and reduces overfitting` by training multiple models on random subsets of data and aggregating their predictions. It is commonly used with **Decision Trees**, but it can be applied to other models as well.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç How Bagging Works Step by Step:**\n",
    "#### **1Ô∏è‚É£ Bootstrap Sampling (Data Resampling) :**\n",
    "- Given a dataset of **N samples**, bagging **randomly selects multiple subsets** (with replacement).\n",
    "- Each subset is the same size as the original dataset but **may have duplicate samples** due to replacement.\n",
    "- On average, each individual subset contains about **63%** of unique samples (some samples appear multiple times, others not at all).\n",
    "\n",
    "#### **2Ô∏è‚É£ Train Multiple Base Models :**\n",
    "- A separate model (often a **Decision Tree**) is trained on **each bootstrapped subset**.\n",
    "- Since each subset is different, the models learn **slightly different patterns**.\n",
    "\n",
    "#### **3Ô∏è‚É£ Aggregate Predictions :**\n",
    "- For **classification**, predictions are combined using **majority voting** (the most common class is chosen).  \n",
    "- For **regression**, predictions are **averaged** to get the final output.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Advantages of Bagging:**\n",
    "‚úÖ **Reduces Overfitting** ‚Äì Each model learns different patterns, preventing high variance.  \n",
    "‚úÖ **Improves Stability** ‚Äì Aggregating predictions makes results **more reliable**.  \n",
    "‚úÖ **Handles Noise Well** ‚Äì Since models see different data, the effect of noise is minimized.  \n",
    "‚úÖ **Works with Weak Models** ‚Äì Even simple models (e.g., Decision Trees) perform well when bagged.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Key Differences: Bagging vs. Boosting-**\n",
    "| Feature | **Bagging** | **Boosting** |\n",
    "|----------|-------------|-------------|\n",
    "| **Training Process** | Models train independently | Models train sequentially, correcting errors |\n",
    "| **Focus** | Reduces variance (overfitting) | Reduces bias (underfitting) |\n",
    "| **Aggregation** | Majority voting (classification), averaging (regression) | Weighted sum of weak models |\n",
    "| **Example** | Random Forest | AdaBoost, XGBoost |\n",
    "\n",
    "---\n",
    "\n",
    "#### **When to Use Bagging?**\n",
    "üîπ When we have **high variance models** (e.g., Decision Trees).  \n",
    "üîπ When we need a **stable and accurate** model.  \n",
    "üîπ When our dataset contains **noise** and you want robustness.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bagging Classifier with Decision Trees\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), \n",
    "                                n_estimators=50,  # Number of models in ensemble\n",
    "                                max_samples=0.8,  # Use 80% of data per bootstrap\n",
    "                                random_state=42)\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ***How do you evaluate a Bagging Classifier‚Äôs performance?***\n",
    "*Answer-*\n",
    "\n",
    "****To evaluate the performance of a Bagging Classifier, we use multiple metrics depending on the problem type `(classification or regression)`. Below are the key steps and methods used.**\n",
    "\n",
    "#### 1. **Train-Test Split and Cross-Validation:**\n",
    "    -   Before evaluating performance, the dataset is splitted into training and testing sts to avoid overfitting. and additionally, we use cross-validation to get a reliable estimate.\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Example in Python**\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Performance Metrics for Classification**  \n",
    "After training, evaluate the model using the following metrics:\n",
    "\n",
    "#### **‚úî Accuracy (Overall Performance)**\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "- Measures how many predictions are correct.\n",
    "- Best for balanced datasets.\n",
    "\n",
    "#### **‚úî Precision, Recall, F1-Score (For Imbalanced Data)**\n",
    "```python\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "- **Precision**: How many predicted positives are correct?  \n",
    "- **Recall**: How many actual positives were correctly identified?  \n",
    "- **F1-score**: Harmonic mean of precision and recall.\n",
    "\n",
    "#### **‚úî ROC-AUC Score (For Probabilistic Models)**\n",
    "If our BaggingClassifier supports probability predictions (`predict_proba`):\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_prob = bagging_clf.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "```\n",
    "- Best for binary classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Performance Metrics for Regression**\n",
    "If using a **Bagging Regressor**, common evaluation metrics include:\n",
    "\n",
    "#### **‚úî Mean Squared Error (MSE)**\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "```\n",
    "- Measures the average squared difference between actual and predicted values.\n",
    "- Lower MSE is better.\n",
    "\n",
    "#### **‚úî R¬≤ Score (Coefficient of Determination)**\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R¬≤ Score:\", r2)\n",
    "```\n",
    "- Measures how well the model explains variance in the data.\n",
    "- Closer to **1** means better performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Cross-Validation for Reliable Performance Measurement**\n",
    "Instead of relying on a single train-test split, use **k-fold cross-validation**:\n",
    "```python\n",
    "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')  # For classification\n",
    "print(\"Cross-Validation Accuracy:\", cv_scores.mean())\n",
    "```\n",
    "- Reduces dependence on a single test set.\n",
    "- Gives a more reliable performance estimate.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Summary: Key Evaluation Metrics**\n",
    "| Task | Primary Metrics | When to Use |\n",
    "|------|---------------|------------|\n",
    "| **Classification** | Accuracy | When classes are balanced |\n",
    "|  | Precision & Recall | When dealing with imbalanced classes |\n",
    "|  | F1-Score | For a balanced tradeoff between precision & recall |\n",
    "|  | ROC-AUC Score | When probability scores matter |\n",
    "| **Regression** | MSE (Mean Squared Error) | When minimizing prediction error is key |\n",
    "|  | R¬≤ Score | When explaining variance is important |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ***How does a Bagging Regressor work?***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "A **Bagging Regressor** is an ensemble learning method that improves the stability and accuracy of regression models by combining multiple versions of the same base model, each trained on different subsets of data. It reduces **variance**, making predictions more robust and resistant to overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç How Bagging Regressor Works: Step-by-Step**\n",
    "#### **1Ô∏è‚É£ Bootstrap Sampling (Data Resampling) üé≤**\n",
    "- Given a dataset of **N samples**, the model randomly selects multiple **bootstrap samples** (subsets of data with replacement).  \n",
    "- Each subset contains **around 63%** of unique samples, and some samples may be repeated.\n",
    "\n",
    "#### **2Ô∏è‚É£ Train Multiple Base Models ü§ñ**\n",
    "- Each subset is used to train a **separate regression model** (commonly Decision Trees, but can be any regressor).\n",
    "- Since each model sees different data, they learn **slightly different patterns**.\n",
    "\n",
    "#### **3Ô∏è‚É£ Aggregate Predictions üìä**\n",
    "- Unlike classification (where we use majority voting), for regression we take the **average** of all individual model predictions:\n",
    "  \n",
    "  \\[\n",
    "  \\hat{y} = \\frac{1}{M} \\sum_{i=1}^{M} \\hat{y}_i\n",
    "  \\]\n",
    "  where \\( M \\) is the number of models, and \\( \\hat{y}_i \\) is the prediction from each model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå Example: Bagging Regressor in Python**\n",
    "```python\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Bagging Regressor with Decision Trees\n",
    "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
    "                               n_estimators=50,  # Number of trees in the ensemble\n",
    "                               max_samples=0.8,  # Use 80% of the data per model\n",
    "                               random_state=42)\n",
    "\n",
    "# Train the model\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R¬≤ Score:\", r2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Advantages of Bagging Regressor**\n",
    "‚úÖ **Reduces Overfitting** ‚Äì Since different models see different data, variance is lower.  \n",
    "‚úÖ **Improves Stability** ‚Äì Aggregating multiple predictions makes results more consistent.  \n",
    "‚úÖ **Works with Weak Models** ‚Äì Even simple regressors (e.g., Decision Trees) perform well.  \n",
    "‚úÖ **Handles Outliers** ‚Äì Since each model is trained on different subsets, the impact of outliers is reduced.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ When to Use Bagging Regressor?**\n",
    "üîπ When our base model **overfits** (e.g., Decision Trees).  \n",
    "üîπ When our dataset contains **noise** or **outliers**.  \n",
    "üîπ When we want to **increase model stability** without adding complexity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ***What is the main advantage of ensemble techniques?***\n",
    "*Answer-*\n",
    "\n",
    "#### üåü **Main Advantage of Ensemble Techniques** üåü  \n",
    "\n",
    "The **primary advantage** of ensemble techniques is that they **combine multiple models** to improve **accuracy, stability, and generalization** compared to using a single model. This helps in reducing **overfitting, bias, and variance**, making predictions more reliable.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Key Benefits of Ensemble Techniques**  \n",
    "\n",
    "#### **1Ô∏è‚É£ Higher Accuracy üìà**\n",
    "‚úÖ By aggregating multiple models, ensembles make more accurate predictions than individual models.  \n",
    "‚úÖ Example: **Random Forest** (an ensemble of Decision Trees) often outperforms a single Decision Tree.  \n",
    "\n",
    "#### **2Ô∏è‚É£ Reduced Overfitting üéØ**\n",
    "‚úÖ Individual models (e.g., deep Decision Trees) can overfit, but ensembles reduce this risk.  \n",
    "‚úÖ Example: **Bagging** reduces variance by training multiple models on different data subsets.  \n",
    "\n",
    "#### **3Ô∏è‚É£ Reduced Variance & Bias ‚öñÔ∏è**\n",
    "‚úÖ **Bagging** decreases **variance** (stabilizing high-variance models like Decision Trees).  \n",
    "‚úÖ **Boosting** reduces **bias** by training models sequentially to correct errors.  \n",
    "\n",
    "#### **4Ô∏è‚É£ More Robust to Noisy Data üîä**\n",
    "‚úÖ Since ensemble models use multiple perspectives, they are **less sensitive to outliers**.  \n",
    "‚úÖ Example: In **Bagging**, some models may not include outliers due to random sampling, reducing their impact.  \n",
    "\n",
    "#### **5Ô∏è‚É£ Works with Weak Models üèóÔ∏è**\n",
    "‚úÖ Even weak models (e.g., shallow Decision Trees) can be **strong when combined**.  \n",
    "‚úÖ Example: **AdaBoost** and **Gradient Boosting** turn weak models into a powerful predictor.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Popular Ensemble Techniques and Their Advantages**\n",
    "| Ensemble Method | Key Advantage |\n",
    "|----------------|--------------|\n",
    "| **Bagging (Bootstrap Aggregation)** | Reduces variance (e.g., Random Forest) |\n",
    "| **Boosting (e.g., AdaBoost, Gradient Boosting)** | Reduces bias by sequentially improving weak models |\n",
    "| **Stacking** | Combines multiple different models for better performance |\n",
    "| **Voting/Averaging** | Aggregates predictions for better accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ When Should You Use Ensemble Techniques?**\n",
    "‚úÖ When a **single model is not performing well**.  \n",
    "‚úÖ When the model is **overfitting or underfitting**.  \n",
    "‚úÖ When we need a **more stable and accurate** prediction.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ***What is the main challenge of ensemble methods?***\n",
    "*Answer-*\n",
    "\n",
    "#### üöß **Main Challenge of Ensemble Methods** üöß  \n",
    "\n",
    "While ensemble methods **improve accuracy and reduce overfitting**, they come with some **challenges and trade-offs**. The **main challenge** is the **increased computational complexity**, but there are other issues as well.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç Key Challenges of Ensemble Methods:**  \n",
    "\n",
    "#### **1Ô∏è‚É£ Higher Computational Cost üíª**\n",
    "‚úÖ **Why?**  \n",
    "- Training multiple models takes **more time and resources** than training a single model.  \n",
    "- Some ensemble methods (e.g., **Boosting**) train models **sequentially**, making them slow.  \n",
    "\n",
    "‚úÖ **Example:**  \n",
    "- A **Random Forest** with 100 trees takes much longer to train than a single **Decision Tree**.  \n",
    "- **Gradient Boosting** needs sequential learning, increasing training time.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Less Interpretability üßê**\n",
    "‚úÖ **Why?**  \n",
    "- Individual models (e.g., a single Decision Tree) are easy to interpret.  \n",
    "- Ensembles (e.g., **Random Forest, XGBoost**) combine many models, making them more of a **black box**.  \n",
    "\n",
    "‚úÖ **Example:**  \n",
    "- A **Decision Tree** can be visualized, but a **Random Forest with 100 trees** is much harder to explain.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Risk of Overfitting (in Some Cases) üéØ**\n",
    "‚úÖ **Why?**  \n",
    "- Some ensemble methods (especially **Boosting**) can still **overfit** if not properly tuned.  \n",
    "- If the ensemble is **too complex**, it may **memorize noise** instead of learning patterns.  \n",
    "\n",
    "‚úÖ **Example:**  \n",
    "- **Gradient Boosting** can overfit if too many trees are used without **regularization**.  \n",
    "\n",
    "‚úÖ **Solution:**  \n",
    "- Use **cross-validation** and **hyperparameter tuning** (like pruning trees or adjusting learning rates).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Difficult to Deploy üöÄ**\n",
    "‚úÖ **Why?**  \n",
    "- A single model is easy to deploy in **real-time applications**.  \n",
    "- A large ensemble (e.g., 100+ models) requires **more memory and computation**, which is challenging in **mobile or embedded systems**.  \n",
    "\n",
    "‚úÖ **Example:**  \n",
    "- A **logistic regression** model runs quickly, but an **ensemble of 500 deep trees** is too slow for real-time predictions.  \n",
    "\n",
    "‚úÖ **Solution:**  \n",
    "- Use **model compression** or **distillation** (training a simpler model to approximate the ensemble).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **5Ô∏è‚É£ More Data Required üìä**\n",
    "‚úÖ **Why?**  \n",
    "- Some ensemble methods, like **Boosting**, require **a lot of data** to generalize well.  \n",
    "- If trained on **small datasets**, they may **not perform better** than a single model.  \n",
    "\n",
    "‚úÖ **Example:**  \n",
    "- **Boosting algorithms (e.g., XGBoost, AdaBoost)** work best with **large datasets**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Summary: Trade-offs of Ensemble Methods**\n",
    "| Challenge | Why It Matters | Example |\n",
    "|-----------|---------------|---------|\n",
    "| **Computational Cost** | Takes longer to train | Random Forest with 100+ trees |\n",
    "| **Less Interpretability** | Hard to explain decisions | Gradient Boosting vs. Decision Tree |\n",
    "| **Overfitting Risk** | Can learn noise if not tuned | XGBoost with too many trees |\n",
    "| **Deployment Issues** | Needs more resources | Large ensemble models in real-time apps |\n",
    "| **More Data Required** | Works best with big datasets | Boosting algorithms need a lot of data |\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• **How to Overcome These Challenges?**\n",
    "‚úÖ **Reduce Model Complexity** (Limit the number of estimators/trees).  \n",
    "‚úÖ **Use Feature Selection** (Reduce input features to speed up training).  \n",
    "‚úÖ **Use Hyperparameter Tuning** (Regularization to prevent overfitting).  \n",
    "‚úÖ **Optimize for Deployment** (Convert large models into smaller, efficient versions).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ***Explain the key idea behind ensemble techniques.***\n",
    "*Answer-*\n",
    "\n",
    "#### üéØ **Key Idea Behind Ensemble Techniques** üéØ  \n",
    "\n",
    "The **core idea** behind ensemble techniques is to **combine multiple models** to create a **stronger, more accurate, and robust** predictive model. By aggregating different models, ensemble methods **reduce errors, improve generalization, and enhance stability** compared to individual models.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç Why Does Ensemble Learning Work?**  \n",
    "\n",
    "#### **1Ô∏è‚É£ Reducing Variance (Bagging) üé≤**  \n",
    "‚úÖ In high-variance models (e.g., **Decision Trees**), predictions can change significantly with small data variations.  \n",
    "‚úÖ **Bagging (Bootstrap Aggregation)** reduces variance by training multiple models on different subsets of data and averaging their predictions.  \n",
    "‚úÖ Example: **Random Forest** (an ensemble of Decision Trees).  \n",
    "\n",
    "#### **2Ô∏è‚É£ Reducing Bias (Boosting) üöÄ**  \n",
    "‚úÖ Weak models (e.g., shallow trees) often have high bias and miss important patterns.  \n",
    "‚úÖ **Boosting** corrects errors **sequentially**, where each new model focuses on improving the mistakes of the previous one.  \n",
    "‚úÖ Example: **Gradient Boosting, AdaBoost, XGBoost**.  \n",
    "\n",
    "#### **3Ô∏è‚É£ Improving Model Diversity (Stacking) üîÄ**  \n",
    "‚úÖ Different models learn different patterns from data.  \n",
    "‚úÖ **Stacking** combines multiple diverse models (**e.g., Decision Trees, SVMs, Neural Networks**) and uses a **meta-model** to improve final predictions.  \n",
    "‚úÖ Example: **StackingClassifier** (combining multiple classifiers).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå Types of Ensemble Techniques**\n",
    "| Ensemble Method | Key Idea | Example |\n",
    "|----------------|---------|---------|\n",
    "| **Bagging** | Reduces variance by training models on different data subsets and averaging predictions | Random Forest |\n",
    "| **Boosting** | Reduces bias by training models sequentially to correct previous mistakes | Gradient Boosting, XGBoost |\n",
    "| **Stacking** | Combines multiple diverse models and uses a meta-model to make final predictions | StackingClassifier |\n",
    "| **Voting/Averaging** | Aggregates predictions from multiple models for better accuracy | Soft/Hard Voting |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Why Use Ensemble Learning?**\n",
    "‚úÖ **Higher Accuracy** ‚Äì Combines multiple models to improve predictive performance.  \n",
    "‚úÖ **More Robustness** ‚Äì Less sensitive to noise and data variations.  \n",
    "‚úÖ **Better Generalization** ‚Äì Avoids overfitting and works well on unseen data.  \n",
    "‚úÖ **Works with Weak Models** ‚Äì Even simple models can perform well when combined.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ***What is a Random Forest Classifier?***\n",
    "*Answer-*\n",
    "\n",
    "####  **Random Forest Classifier: An Overview**\n",
    "\n",
    "A **Random Forest Classifier** is an **ensemble learning algorithm** that combines multiple **Decision Trees** to improve accuracy, reduce overfitting, and create a more stable model. It is widely used for **classification** and **regression** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç How Does a Random Forest Classifier Work?**\n",
    "#### **1Ô∏è‚É£ Bootstrap Sampling (Bagging) üé≤**\n",
    "- The algorithm randomly selects multiple **subsets** of training data (with replacement).  \n",
    "- Each subset is used to train an **individual Decision Tree**.  \n",
    "\n",
    "#### **2Ô∏è‚É£ Feature Randomness üèÜ**\n",
    "- Each tree considers a **random subset of features** when splitting nodes, ensuring diversity.  \n",
    "- This prevents trees from relying too much on specific features.  \n",
    "\n",
    "#### **3Ô∏è‚É£ Aggregation (Voting) üó≥Ô∏è**\n",
    "- Each tree makes a **prediction**, and the final output is based on **majority voting**.  \n",
    "- In regression tasks, predictions are **averaged** instead of voting.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Advantages of Random Forest**\n",
    "‚úÖ **High Accuracy** ‚Äì Works well for classification and regression tasks.  \n",
    "‚úÖ **Reduces Overfitting** ‚Äì Unlike a single Decision Tree, Random Forest is more stable.  \n",
    "‚úÖ **Handles Missing Data** ‚Äì Can work with missing values and noisy data.  \n",
    "‚úÖ **Feature Importance** ‚Äì Can rank features based on importance.  \n",
    "‚úÖ **Scalability** ‚Äì Works well on large datasets with many features.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ When to Use Random Forest?**\n",
    "‚úÖ When we need a **highly accurate** and **robust** model.  \n",
    "‚úÖ When a **single Decision Tree overfits** your data.  \n",
    "‚úÖ When working with **large datasets** with many features.  \n",
    "‚úÖ When feature selection is important (Random Forest provides feature importance scores).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. ***What are the main types of ensemble techniques?***\n",
    "*Answer-*\n",
    "\n",
    "#### üéØ **Main Types of Ensemble Techniques** üéØ  \n",
    "\n",
    "Ensemble techniques **combine multiple models** to improve accuracy, stability, and generalization. There are **four main types** of ensemble learning methods:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1Ô∏è‚É£ Bagging (Bootstrap Aggregation) üé≤**  \n",
    "‚úÖ **Goal:** Reduce **variance** and improve stability.  \n",
    "‚úÖ **How it Works:**  \n",
    "- Multiple models (usually the same type) are trained on **different random subsets** of data.  \n",
    "- Final prediction is made by **majority voting (classification)** or **averaging (regression)**.  \n",
    "‚úÖ **Example:** **Random Forest (multiple Decision Trees)**.  \n",
    "\n",
    "üîπ **Python Example:**  \n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Boosting üöÄ**  \n",
    "‚úÖ **Goal:** Reduce **bias** by learning from mistakes of previous models.  \n",
    "‚úÖ **How it Works:**  \n",
    "- Models are trained **sequentially**, where each new model corrects the errors of the previous ones.  \n",
    "- Assigns **higher weights** to misclassified data points to focus on difficult cases.  \n",
    "‚úÖ **Examples:** **AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost**.  \n",
    "\n",
    "üîπ **Python Example (AdaBoost):**  \n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Stacking (Stacked Generalization) üîÄ**  \n",
    "‚úÖ **Goal:** Combine predictions from **different types of models**.  \n",
    "‚úÖ **How it Works:**  \n",
    "- Multiple diverse models (e.g., Decision Trees, SVM, Neural Networks) make predictions.  \n",
    "- A **meta-model (final model)** learns from these predictions to improve accuracy.  \n",
    "‚úÖ **Example:** Combining **Logistic Regression, Decision Trees, and Neural Networks**.  \n",
    "\n",
    "üîπ **Python Example (Stacking Classifier):**  \n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True))],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Voting (Majority/Averaging) üó≥Ô∏è**  \n",
    "‚úÖ **Goal:** Aggregate predictions from multiple models for a better result.  \n",
    "‚úÖ **How it Works:**  \n",
    "- **Hard Voting:** Each model votes, and the majority class is chosen.  \n",
    "- **Soft Voting:** Uses probabilities and selects the class with the highest sum of probabilities.  \n",
    "‚úÖ **Example:** Combining **Random Forest, SVM, and Logistic Regression**.  \n",
    "\n",
    "üîπ **Python Example (Voting Classifier):**  \n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', LogisticRegression()), ('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True))],\n",
    "    voting='soft'  # Use 'hard' for majority voting\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ Summary Table of Ensemble Techniques**\n",
    "| Method | Key Idea | Goal | Example |\n",
    "|--------|---------|------|---------|\n",
    "| **Bagging** | Train models in parallel on different data subsets | Reduce variance | Random Forest |\n",
    "| **Boosting** | Train models sequentially, correcting previous errors | Reduce bias | AdaBoost, XGBoost |\n",
    "| **Stacking** | Combine different models & use a meta-model | Improve accuracy | StackingClassifier |\n",
    "| **Voting** | Aggregate predictions from multiple models | Improve robustness | VotingClassifier |\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ **Which Ensemble Method Should You Use?**\n",
    "‚úÖ **Use Bagging** if the model **overfits** (high variance).  \n",
    "‚úÖ **Use Boosting** if the model has **high bias** (low complexity models).  \n",
    "‚úÖ **Use Stacking** if the want to **combine diverse models** for better accuracy.  \n",
    "‚úÖ **Use Voting** if we have **multiple strong models** with different strengths.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. ***What is ensemble learning in machine learning?***\n",
    "*Answer-*\n",
    "\n",
    "#### üéØ **What is Ensemble Learning in Machine Learning?**  \n",
    "\n",
    "**Ensemble learning** is a machine learning technique where multiple models (often called **\"weak learners\"**) are combined to make better predictions. Instead of relying on a single model, ensemble learning **aggregates the predictions** of multiple models to improve accuracy, reduce overfitting, and enhance generalization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Why is Ensemble Learning Important?**  \n",
    "‚úî **Improves accuracy:** Reduces individual model errors by combining multiple predictions.  \n",
    "‚úÖ **Reduces Overfitting** ‚Äì Prevents the model from memorizing noise in the data.  \n",
    "‚öñ **Reduces Variance & Bias** ‚Äì Handles underfitting (high bias) and overfitting (high variance).  \n",
    "üîÑ **Works for Different Models** ‚Äì Can combine multiple algorithms to leverage their strengths.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç How Does Ensemble Learning Work?**  \n",
    "Ensemble learning combines multiple base models (weak learners) to create a more accurate and robust **final model**. The key idea is that multiple weak models, when combined, outperform a single strong model.\n",
    "\n",
    "#### **Types of Ensemble Learning Methods:**\n",
    "\n",
    "#### **1Ô∏è‚É£ Bagging (Bootstrap Aggregation) üèó**\n",
    "üëâ **Purpose:** Reduces **variance**, prevents overfitting.  \n",
    "üëâ **How it works:**  \n",
    "- Trains multiple models on **random subsets** (with replacement) of the dataset.  \n",
    "- Each model votes for the final output (for classification) or averages predictions (**for regression**).  \n",
    "\n",
    "‚úÖ Example: **Random Forest (Ensemble of Decision Trees with bootstrapping)**  \n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Boosting ‚ö°**\n",
    "üëâ **Purpose:** Reduces **bias** by training new models to **correct errors** of previous models.  \n",
    "üëâ **How it works:**  \n",
    "- Models are trained **sequentially**, each focusing on errors made by the previous model.  \n",
    "- It gives higher weight to misclassified points in each iteration.  \n",
    "\n",
    "#### **Popular Boosting Methods:**\n",
    "- **AdaBoost:** Corrects mistakes by focusing more on misclassified examples.  \n",
    "- **Gradient Boosting (GBM):** Builds trees sequentially, correcting the mistakes of previous trees.  \n",
    "- **XGBoost, LightGBM, CatBoost:** Optimized versions of boosting algorithms for better performance.  \n",
    "\n",
    "‚úÖ **Great for:** Handling complex patterns and reducing both **bias & variance**.  \n",
    "‚ö†Ô∏è **Caution:** Can overfit if not properly tuned.  \n",
    "\n",
    "üîπ **Example using AdaBoost in Python:**  \n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, learning_rate=0.1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Stacking (Stacked Generalization) üèóÔ∏èüîÄ**\n",
    "üëâ **Purpose:** Combine multiple diverse models to improve predictive power.  \n",
    "üëâ **How it works:**  \n",
    "- Uses multiple models (Decision Trees, Neural Networks, SVM, etc.) as **base learners**.  \n",
    "- A **meta-model** (often a simple model like Logistic Regression) is trained to make the final prediction based on the predictions of the base models.  \n",
    "\n",
    "‚úÖ **Great for:** Complex problems where different models perform well on different parts of the data.  \n",
    "‚ùó **Can be computationally expensive.**  \n",
    "\n",
    "üîπ **Python Example (Stacking Classifier):**  \n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('rf', RandomForestClassifier()), ('svm', SVC(probability=True))],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Voting & Averaging üó≥**\n",
    "üëâ **Purpose:** Improve prediction by combining outputs of multiple models.  \n",
    "‚úÖ **How it works:**  \n",
    "- **Hard Voting:** Each model votes for a class, and the majority wins.  \n",
    "- **Soft Voting:** The probabilities from all models are averaged, and the class with the highest probability is selected.  \n",
    "\n",
    "‚úÖ **Great for:** Combining multiple models to improve accuracy.  \n",
    "\n",
    "üîπ **Python Example (Voting Classifier):**  \n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define base models\n",
    "lr = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "# Create VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('dt', dt), ('svm', svm)],\n",
    "    voting='soft'  # Use 'hard' for majority voting\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ When to Use Ensemble Learning?**\n",
    "‚úî **Large Datasets** ‚Äì More data allows multiple models to learn better.  \n",
    "‚úî **High Variance Models (Overfitting)** ‚Äì Bagging helps with Decision Trees, Random Forests.  \n",
    "‚úî **High Bias Models** ‚Äì Use Boosting to improve weak learners.  \n",
    "‚úî **High-Stakes Predictions** ‚Äì Like medical diagnosis, fraud detection, etc.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Conclusion**  \n",
    "‚úÖ **Ensemble learning is a powerful technique** that improves model performance.  \n",
    "‚úÖ It can reduce **bias, variance, and overfitting**.  \n",
    "‚úÖ **Bagging, Boosting, and Stacking** are the main approaches.  \n",
    "‚úÖ Used in real-world applications like **fraud detection, medical diagnosis, finance, recommendation systems, and competitions (Kaggle)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. ***When should we avoid using ensemble methods?***\n",
    "*Answer-*\n",
    "\n",
    "#### **When Should You Avoid Using Ensemble Methods?**\n",
    "\n",
    "While **ensemble methods** can significantly improve model performance, they are **not always the best choice**. Here are some scenarios where using ensemble learning **might not be ideal**:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1Ô∏è‚É£ When Interpretability is Important üîç**  \n",
    "‚úÖ **Problem:** Ensemble models (e.g., Random Forest, Boosting, Stacking) create **complex, black-box models** that are difficult to interpret.  \n",
    "‚úÖ **Example:**  \n",
    "- In medical or financial applications, where clear decision-making is required.  \n",
    "- A **Decision Tree** or **Logistic Regression** might be preferable.  \n",
    "\n",
    "‚úî **Alternative:** Use simpler models like **Decision Trees, Logistic Regression, or Explainable AI (SHAP, LIME)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ When Training Time & Resources Are Limited ‚è≥üíª**  \n",
    "‚úÖ **Problem:** Ensemble methods (especially **Boosting & Stacking**) are computationally expensive.  \n",
    "‚úÖ **Example:**  \n",
    "- **XGBoost, LightGBM, CatBoost** require significant computation, making them **slow on large datasets**.  \n",
    "- Training a **large Random Forest or a Stacking model** can be resource-intensive.  \n",
    "\n",
    "‚úî **Alternative:** Use a **single, well-tuned model** (e.g., Decision Tree, SVM, or a simple Neural Network).\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ When Your Dataset is Small üìâ**  \n",
    "‚úÖ **Problem:** Ensemble models require **enough diverse data** to be effective.  \n",
    "‚úÖ **Example:**  \n",
    "- If you only have **a few hundred samples**, bagging (like Random Forest) may **overfit** instead of improving performance.  \n",
    "\n",
    "‚úî **Alternative:** Use **simple models** like Decision Trees, Logistic Regression, or KNN.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ When a Single Model Performs Well üí™**  \n",
    "‚úÖ **Problem:** If a **single model already achieves high accuracy**, adding an ensemble may not provide significant benefits.  \n",
    "‚úÖ **Example:**  \n",
    "- If a **Logistic Regression or SVM** achieves 98% accuracy, a Random Forest might not add much improvement.  \n",
    "\n",
    "‚úî **Alternative:** Stick with the **best single model** and fine-tune hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5Ô∏è‚É£ When You Need Real-Time Predictions ‚ö°**  \n",
    "‚úÖ **Problem:** Some ensemble models (**Boosting, Stacking, Large Random Forests**) have **slow inference times**.  \n",
    "‚úÖ **Example:**  \n",
    "- **Self-driving cars, fraud detection, and recommendation systems** require real-time predictions, where ensemble methods might be **too slow**.  \n",
    "\n",
    "‚úî **Alternative:** Use **lighter models** or **reduce the number of estimators** in ensembles.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Summary: When to Avoid Ensemble Learning**\n",
    "| ‚ùå **Situation** | ‚ùó **Why?** | ‚úÖ **Alternative** |\n",
    "|---------------|----------|---------------|\n",
    "| **Interpretability is crucial** | Ensembles are complex black-box models | Use Decision Trees, Logistic Regression |\n",
    "| **Limited computational power** | Boosting & Stacking require high resources | Use simpler models like SVM |\n",
    "| **Small dataset** | Ensembles may overfit | Use Logistic Regression, KNN |\n",
    "| **Single model performs well** | No significant gain from ensembles | Fine-tune the best model |\n",
    "| **Real-time predictions needed** | Ensembles can be slow | Use lightweight models |\n",
    "\n",
    "---\n",
    "\n",
    "####  **Final Thoughts**  \n",
    "‚úî **Ensemble learning is powerful**, but it's **not always necessary**.  \n",
    "‚úî Always **analyze your problem** before applying an ensemble method.  \n",
    "‚úî Sometimes, **a well-tuned single model** is the best choice!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. ***How does Bagging help in reducing overfitting?***\n",
    "*Answer-*\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)** is an ensemble learning technique that helps **reduce overfitting** by training multiple models independently on different random subsets of the data and then combining their predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üîç Why Does Overfitting Happen?**  \n",
    "Overfitting occurs when a model **learns noise and specific details** from the training data, leading to poor generalization on new data. This is common in **high-variance models** like **Decision Trees**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üõ† How Bagging Works to Reduce Overfitting**\n",
    "#### **1Ô∏è‚É£ Bootstrap Sampling (Data Randomization) üé≤**\n",
    "- Bagging **creates multiple random subsets** of the training data **with replacement** (bootstrap sampling).  \n",
    "- Each model (weak learner) is trained on **a different subset**, introducing diversity in learning.  \n",
    "- Since the models see **different variations of the data**, they don‚Äôt overfit to a specific training set.  \n",
    "\n",
    "üëâ **Effect:** Reduces model variance by ensuring each model captures a slightly different pattern.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Independent Model Training üèó**\n",
    "- Each model is trained **independently** on its subset.  \n",
    "- Unlike boosting (where models learn sequentially), **bagging allows parallel training**, avoiding **bias propagation**.  \n",
    "\n",
    "üëâ **Effect:** Encourages independence between models, reducing overfitting.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Aggregation (Majority Voting / Averaging) üó≥**\n",
    "- For **classification**, each model votes for a class, and the **majority vote** determines the final prediction.  \n",
    "- For **regression**, predictions are **averaged** to smooth out individual model errors.  \n",
    "\n",
    "üëâ **Effect:** Averages out noise and prevents over-reliance on any single model‚Äôs mistakes.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Reducing Sensitivity to Outliers ‚ö†**\n",
    "- Since different models see different data, extreme outliers in one subset do not significantly affect the overall prediction.  \n",
    "- Individual models may overfit, but their combined decision smooths out anomalies.  \n",
    "\n",
    "üëâ **Effect:** More **robust** predictions, reducing the impact of noise and overfitting.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Key Takeaways**\n",
    "‚úÖ **Reduces Overfitting:** By training on different data subsets, preventing a single model from memorizing noise.  \n",
    "‚úÖ **Decreases Variance:** Combining multiple models smooths out individual model errors.  \n",
    "‚úÖ **More Robust to Outliers:** Reduces sensitivity to noise by aggregating predictions.  \n",
    "‚úÖ **Works Well with High-Variance Models:** **Decision Trees, Neural Networks, etc.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bagging with Decision Trees\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(), \n",
    "    n_estimators=100, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. ***Why is Random Forest better than a single Decision Tree?***\n",
    "*Answer-*\n",
    " \n",
    "\n",
    "A **Random Forest** is an **ensemble of multiple Decision Trees**, which improves performance by reducing **overfitting**, increasing **stability**, and enhancing **generalization**. Here's why Random Forest is superior to a single Decision Tree:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1Ô∏è‚É£ Reduces Overfitting (Lower Variance) üöÄ**  \n",
    "- A **single Decision Tree** often **memorizes** training data, leading to **overfitting** (high variance).  \n",
    "- **Random Forest reduces overfitting** by training **multiple Decision Trees on different random subsets** of data and features.  \n",
    "- The final prediction is based on **majority voting (classification)** or **averaging (regression)**, smoothing out extreme predictions.  \n",
    "\n",
    "üîπ **Example:**  \n",
    "A Decision Tree might classify all training samples correctly but **fail on new test data**. Random Forest generalizes better, avoiding this issue.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ More Stable & Robust üèó**  \n",
    "- A **single Decision Tree** is sensitive to small data changes‚Äîchanging just a few samples can **completely alter** the tree structure.  \n",
    "- **Random Forest is more stable** since multiple trees vote, making the model less sensitive to fluctuations in the dataset.  \n",
    "\n",
    "üîπ **Example:**  \n",
    "If you train multiple Decision Trees on slightly different training samples, they **produce very different structures**. But Random Forest smooths out these differences.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Handles Missing Data & Outliers Better üîç**  \n",
    "- A **single Decision Tree** can be **misled by outliers**, as it tries to fit the training data as perfectly as possible.  \n",
    "- **Random Forest reduces the impact of outliers** since each tree is trained on different subsets of data‚Äîso no single outlier dominates the learning process.  \n",
    "\n",
    "üîπ **Example:**  \n",
    "A **single Decision Tree** might create a deep, complex structure to fit an outlier, but **Random Forest averages out this effect**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Works Well with High-Dimensional Data (Feature Randomness) üé≤**  \n",
    "- **A Decision Tree considers all features** at each split, which can lead to overfitting.  \n",
    "- **Random Forest randomly selects a subset of features** at each split, making the model more **diverse and generalized**.  \n",
    "\n",
    "üîπ **Example:**  \n",
    "If 90% of the features are irrelevant, a **Decision Tree might still use them**. But Random Forest ignores many irrelevant features, improving efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5Ô∏è‚É£ Parallel Processing & Scalability ‚ö°**  \n",
    "- A **single Decision Tree grows sequentially**, but **Random Forest can train multiple trees in parallel**, making it faster on large datasets.  \n",
    "- Since each tree is independent, Random Forest **scales well across multiple CPUs or GPUs**.  \n",
    "\n",
    "üîπ **Example:**  \n",
    "On a **large dataset**, training a single **deep** Decision Tree can be **slow and memory-intensive**. Random Forest can distribute the workload across multiple cores.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6Ô∏è‚É£ Provides Feature Importance üìä**  \n",
    "- **Random Forest can rank features** based on their importance in making predictions.  \n",
    "- This is useful for **feature selection** and understanding which variables are most influential.  \n",
    "\n",
    "üîπ **Example:**  \n",
    "In a **medical diagnosis problem**, Random Forest might reveal that **blood pressure** and **age** are more important than other features.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå Quick Comparison: Random Forest vs. Decision Tree**\n",
    "| **Feature**        | **Decision Tree üå≥** | **Random Forest üå≤üå≤üå≤** |\n",
    "|--------------------|--------------------|------------------------|\n",
    "| **Overfitting**   | High (prone to memorization) | Low (generalizes well) |\n",
    "| **Stability**     | Sensitive to small data changes | Robust against variations |\n",
    "| **Handling Outliers** | Easily affected | More resilient |\n",
    "| **Handling Missing Data** | Struggles | Performs better |\n",
    "| **Feature Selection** | Uses all features | Uses random subsets |\n",
    "| **Parallel Processing** | No | Yes (Faster for large data) |\n",
    "| **Feature Importance** | Not available | Provides feature ranking |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Conclusion: Why Choose Random Forest?**\n",
    "‚úÖ **Better generalization** ‚Äì avoids overfitting.  \n",
    "‚úÖ **More stable** ‚Äì less sensitive to small data changes.  \n",
    "‚úÖ **Handles missing values & outliers better.**  \n",
    "‚úÖ **Scales well with large datasets.**  \n",
    "‚úÖ **Provides feature importance rankings.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.875\n",
      "Random Forest Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "\n",
    "# Train a Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. ***What is the role of bootstrap sampling in Bagging?***\n",
    "*Answer-*\n",
    "\n",
    "####\n",
    " üéØ **Role of Bootstrap Sampling in Bagging** üéØ  \n",
    "\n",
    "**Bootstrap Sampling** is the foundation of **Bagging (Bootstrap Aggregating)** and plays a crucial role in reducing **overfitting** and improving model stability. It allows multiple models to be trained on **different subsets** of the data, introducing **diversity** and reducing variance.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå What is Bootstrap Sampling?**  \n",
    "Bootstrap Sampling is a technique where we **randomly sample the training data with replacement** to create multiple different datasets for training multiple models.  \n",
    "\n",
    "- Each **bootstrap sample** has the **same size as the original dataset** but includes **duplicate** samples and **excludes some original samples**.  \n",
    "- This means **each model sees a slightly different dataset**, leading to **diversity in predictions**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üé≤ How Bootstrap Sampling Works in Bagging?**  \n",
    "\n",
    "1Ô∏è‚É£ **Create multiple random subsets** of training data (**with replacement**).  \n",
    "2Ô∏è‚É£ Train **a separate model** (e.g., Decision Tree) on each subset.  \n",
    "3Ô∏è‚É£ **Aggregate predictions** (Majority voting for classification, Averaging for regression).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üî• Why is Bootstrap Sampling Important in Bagging?**  \n",
    "\n",
    "#### **1Ô∏è‚É£ Reduces Overfitting üìâ**\n",
    "- Training multiple models on different datasets prevents over-reliance on specific patterns.  \n",
    "- This helps to **smooth out noise** and **improves generalization** to unseen data.  \n",
    "\n",
    "üëâ **Effect:** Prevents models from memorizing training data.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2Ô∏è‚É£ Reduces Variance (Improves Stability) üîÑ**\n",
    "- Since each model is trained on a slightly different dataset, their errors are **uncorrelated**.  \n",
    "- Aggregating multiple predictions **averages out individual model errors**, making the final model **more stable**.  \n",
    "\n",
    "üëâ **Effect:** Bagging **lowers variance**, preventing drastic fluctuations in predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3Ô∏è‚É£ Increases Diversity Among Models üé≠**\n",
    "- Different training datasets cause models to **learn different decision boundaries**, making the final ensemble **more robust**.  \n",
    "- This prevents **all models from making the same mistakes**.  \n",
    "\n",
    "üëâ **Effect:** Diversity ensures that **one model‚Äôs weakness is compensated by others**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4Ô∏è‚É£ Out-of-Bag (OOB) Error Estimation üéØ**\n",
    "- Since **some data points are left out** in each bootstrap sample, we can use **these unseen points** to estimate error **without needing a separate validation set**.  \n",
    "- This is called **OOB Score**, a built-in validation technique in Bagging.  \n",
    "\n",
    "üëâ **Effect:** OOB Score **reduces the need for a validation split**, maximizing data usage.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **üöÄ Final Summary: Why Bootstrap Sampling is Crucial in Bagging?**\n",
    "‚úÖ **Prevents overfitting** ‚Äì Different models see different data, avoiding memorization.  \n",
    "‚úÖ **Reduces variance** ‚Äì Aggregating multiple models smooths out individual errors.  \n",
    "‚úÖ **Increases model diversity** ‚Äì Ensures that different trees make different decisions.  \n",
    "‚úÖ **Enables OOB scoring** ‚Äì Estimates model performance **without needing a separate validation set**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:  [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Bootstrap Sample:  [ 7  4  8  5  7 10  3  7  8  5]\n"
     ]
    }
   ],
   "source": [
    "#Example: Bootstrap Sampling in Python\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Original dataset (10 samples)\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Bootstrap sample (random sampling with replacement)\n",
    "bootstrap_sample = resample(data, replace=True, n_samples=len(data), random_state=42)\n",
    "\n",
    "print(\"Original Data: \", data)\n",
    "print(\"Bootstrap Sample: \", bootstrap_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "#Example: Bagging Classifier Using Bootstrap Sampling\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Bagging Classifier with Bootstrap Sampling\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    bootstrap=True,  # Enable Bootstrap Sampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. ***What are some real-world applications of ensemble techniques?***\n",
    "*Answer-*\n",
    "\n",
    "#### üéØ **Real-World Applications of Ensemble Techniques**  \n",
    "\n",
    "Ensemble learning techniques like **Bagging, Boosting, and Stacking** are widely used across industries to **improve accuracy, reduce overfitting, and enhance model robustness**. Here are some **real-world applications** where ensemble methods shine:  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 1Ô∏è‚É£ Fraud Detection (Finance & Banking) üí∞**  \n",
    "**Why?** Fraudulent transactions are rare but highly impactful. **Ensemble models** help detect fraud more accurately.  \n",
    "üîπ **Techniques Used:** Random Forest, XGBoost, Stacking  \n",
    "üîπ **Example:**  \n",
    "- Credit card fraud detection (Visa, Mastercard, PayPal)  \n",
    "- Insurance fraud detection  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 2Ô∏è‚É£ Customer Churn Prediction (Telecom, E-commerce) üìûüõí**  \n",
    "**Why?** Predicting when a customer might leave (churn) helps businesses retain customers.  \n",
    "üîπ **Techniques Used:** Bagging (Random Forest), Boosting (Gradient Boosting, AdaBoost)  \n",
    "üîπ **Example:**  \n",
    "- Telecom companies (AT&T, Verizon) predicting **churn risk**  \n",
    "- E-commerce platforms (Amazon, eBay) optimizing **customer retention strategies**  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 3Ô∏è‚É£ Medical Diagnosis & Disease Prediction (Healthcare) üè•**  \n",
    "**Why?** Ensemble models improve medical predictions, leading to **better diagnostics and early detection**.  \n",
    "üîπ **Techniques Used:** Random Forest, XGBoost, Stacking  \n",
    "üîπ **Example:**  \n",
    "- **Cancer detection** using medical imaging  \n",
    "- **Heart disease prediction** based on patient data  \n",
    "- **COVID-19 severity prediction** using CT scans  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 4Ô∏è‚É£ Stock Market Prediction (Finance) üìà**  \n",
    "**Why?** Financial markets are highly volatile, and ensemble models help **reduce prediction errors**.  \n",
    "üîπ **Techniques Used:** Bagging (Random Forest), Boosting (XGBoost, LightGBM)  \n",
    "üîπ **Example:**  \n",
    "- Predicting **stock prices** using historical data  \n",
    "- Algorithmic trading & **portfolio optimization**  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 5Ô∏è‚É£ Image & Object Recognition (Computer Vision) üì∏**  \n",
    "**Why?** Ensemble models improve accuracy in **image classification & object detection** tasks.  \n",
    "üîπ **Techniques Used:** Stacking, Boosting (XGBoost, LightGBM)  \n",
    "üîπ **Example:**  \n",
    "- **Facial recognition** (Apple Face ID, Amazon Rekognition)  \n",
    "- **Self-driving cars** (Tesla, Waymo) recognizing obstacles  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 6Ô∏è‚É£ Spam Detection & Cybersecurity (Tech) üîê**  \n",
    "**Why?** Spam filters and cybersecurity systems rely on ensembles for **better threat detection**.  \n",
    "üîπ **Techniques Used:** Random Forest, AdaBoost, Stacking  \n",
    "üîπ **Example:**  \n",
    "- **Email spam detection** (Gmail, Outlook)  \n",
    "- **Malware detection** (Antivirus software)  \n",
    "- **Intrusion detection systems (IDS)** to prevent hacking attempts  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 7Ô∏è‚É£ Recommender Systems (Streaming & Retail) üé•üõç**  \n",
    "**Why?** Personalized recommendations boost user engagement and sales.  \n",
    "üîπ **Techniques Used:** Stacking, Boosting (XGBoost)  \n",
    "üîπ **Example:**  \n",
    "- **Netflix, YouTube, Spotify** recommending movies/songs  \n",
    "- **Amazon, Walmart** suggesting products based on past behavior  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå 8Ô∏è‚É£ Natural Language Processing (Chatbots, Sentiment Analysis) üó£üìú**  \n",
    "**Why?** NLP tasks require high accuracy, which ensemble models provide.  \n",
    "üîπ **Techniques Used:** Stacking, Boosting (XGBoost, LightGBM)  \n",
    "üîπ **Example:**  \n",
    "- **Chatbots & virtual assistants** (Siri, Alexa, ChatGPT)  \n",
    "- **Sentiment analysis** for customer feedback (Twitter, Amazon reviews)  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Summary: Why Use Ensemble Learning?**\n",
    "‚úÖ **Higher Accuracy** ‚Äì Combines multiple models for better predictions  \n",
    "‚úÖ **Reduces Overfitting** ‚Äì More generalized and robust models  \n",
    "‚úÖ **Better Handling of Imbalanced Data** ‚Äì Useful in fraud detection, medical diagnosis .\n",
    "‚úÖ **Works Well for Both Classification & Regression** ‚Äì Widely applicable across domains .\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. ***What is the difference between Bagging and Boosting?***\n",
    "*Answer-*\n",
    "\n",
    "#### üéØ **Bagging vs. Boosting: Key Differences Explained**  \n",
    "\n",
    "Both **Bagging** and **Boosting** are ensemble learning techniques, but they work in different ways to improve model performance. Let's break it down:  \n",
    "\n",
    "---\n",
    "\n",
    "#### **üîπ 1. Definition**\n",
    "| **Technique** | **Definition** |\n",
    "|-------------|----------------|\n",
    "| **Bagging (Bootstrap Aggregating)** | Trains multiple models independently on random subsets of data (with replacement) and aggregates their predictions (majority voting for classification, averaging for regression). |\n",
    "| **Boosting** | Trains models sequentially, where each new model **focuses on correcting errors** made by the previous models. Final predictions are made by weighted combination of weak learners. |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîπ 2. How They Work**\n",
    "| **Aspect**   | **Bagging** | **Boosting** |\n",
    "|-------------|------------|--------------|\n",
    "| **Model Training** | Multiple models trained **independently** in parallel. | Models trained **sequentially**, with each model improving on the mistakes of the previous one. |\n",
    "| **Data Sampling** | Bootstrap sampling (random sampling **with replacement**). | No bootstrap; instead, weights are adjusted so that misclassified data gets **higher importance**. |\n",
    "| **Aggregation** | Majority voting (classification) or averaging (regression). | Weighted combination of models‚Äô outputs. |\n",
    "| **Focus** | Reduces **variance** (helps in overfitting problems). | Reduces **bias** (helps in underfitting problems). |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîπ 3. Strengths and Weaknesses**\n",
    "| **Aspect** | **Bagging** | **Boosting** |\n",
    "|-----------|------------|-------------|\n",
    "| **Best for** | High variance models (like Decision Trees). | High bias models (like weak learners). |\n",
    "| **Overfitting Risk** | Lower, since models are independent. | Higher, since models adapt too much to training data. |\n",
    "| **Training Speed** | Faster (models trained in parallel). | Slower (sequential training). |\n",
    "| **Handling Noisy Data** | More robust to noise. | Sensitive to outliers (since it gives more weight to difficult cases). |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîπ 4. Algorithms Using Each Technique**\n",
    "| **Bagging Methods** | **Boosting Methods** |\n",
    "|---------------------|----------------------|\n",
    "| **Random Forest** | **AdaBoost (Adaptive Boosting)** |\n",
    "| **Bagging Classifier (Scikit-learn)** | **Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)** |\n",
    "| **ExtraTrees (Extremely Randomized Trees)** | **LogitBoost, LPBoost** |\n",
    "\n",
    "---\n",
    "\n",
    "#### **üöÄ Summary: When to Use Which?**\n",
    "| **Scenario** | **Use Bagging** | **Use Boosting** |\n",
    "|-------------|----------------|------------------|\n",
    "| **Overfitting (High Variance)?** | ‚úÖ Yes, Bagging helps reduce variance. | ‚ùå No, Boosting may overfit more. |\n",
    "| **Underfitting (High Bias)?** | ‚ùå No, Bagging does not help much. | ‚úÖ Yes, Boosting helps reduce bias. |\n",
    "| **Training Speed?** | ‚úÖ Faster (parallel execution). | ‚ùå Slower (sequential execution). |\n",
    "| **Noise Sensitivity?** | ‚úÖ Robust to noise. | ‚ùå Sensitive to noisy data & outliers. |\n",
    "| **Performance on Complex Problems?** | üîπ Works well but may not always be the best. | ‚úÖ Often gives superior performance. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "#Bagging example using random forest>>\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "#Boosting example using AdaBoost>>\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Train AdaBoost Classifier\n",
    "boosting_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42)\n",
    "boosting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = boosting_clf.predict(X_test)\n",
    "print(\"Boosting Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practical:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "#21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a Bagging Classifier with Decision Trees\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 6967.863746774799\n"
     ]
    }
   ],
   "source": [
    "#22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Bagging Regressor with Decision Trees\n",
    "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "\n",
    "# Compute and print Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Bagging Regressor MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.9649122807017544\n",
      "Feature Importances:\n",
      "mean radius: 0.0487\n",
      "mean texture: 0.0136\n",
      "mean perimeter: 0.0533\n",
      "mean area: 0.0476\n",
      "mean smoothness: 0.0073\n",
      "mean compactness: 0.0139\n",
      "mean concavity: 0.0680\n",
      "mean concave points: 0.1062\n",
      "mean symmetry: 0.0038\n",
      "mean fractal dimension: 0.0039\n",
      "radius error: 0.0201\n",
      "texture error: 0.0047\n",
      "perimeter error: 0.0113\n",
      "area error: 0.0224\n",
      "smoothness error: 0.0043\n",
      "compactness error: 0.0053\n",
      "concavity error: 0.0094\n",
      "concave points error: 0.0035\n",
      "symmetry error: 0.0040\n",
      "fractal dimension error: 0.0053\n",
      "worst radius: 0.0780\n",
      "worst texture: 0.0217\n",
      "worst perimeter: 0.0671\n",
      "worst area: 0.1539\n",
      "worst smoothness: 0.0106\n",
      "worst compactness: 0.0203\n",
      "worst concavity: 0.0318\n",
      "worst concave points: 0.1447\n",
      "worst symmetry: 0.0101\n",
      "worst fractal dimension: 0.0052\n"
     ]
    }
   ],
   "source": [
    "#23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy)\n",
    "\n",
    "# Print feature importance scores\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(data.feature_names, rf_clf.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor MSE: 20519.297540712625\n",
      "Random Forest Regressor MSE: 7055.507694741972\n"
     ]
    }
   ],
   "source": [
    "#24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred_dt = dt_reg.predict(X_test)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "# Compute and compare MSE\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Decision Tree Regressor MSE:\", mse_dt)\n",
    "print(\"Random Forest Regressor MSE:\", mse_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Bag (OOB) Score: 0.7830703020923468\n"
     ]
    }
   ],
   "source": [
    "#25. Compute the Out-of-Bag (OOB) Score for a Random Forest Regressor\n",
    "\n",
    "rf_reg_oob = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_reg_oob.fit(X_train, y_train)\n",
    "\n",
    "print(\"Out-of-Bag (OOB) Score:\", rf_reg_oob.oob_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with SVM Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "#26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Bagging Classifier with SVM\n",
    "bagging_svm = BaggingClassifier(estimator=SVC(probability=True), n_estimators=50, random_state=42)\n",
    "bagging_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = bagging_svm.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Classifier with SVM Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 10 trees - Accuracy: 0.956140350877193\n",
      "Random Forest with 50 trees - Accuracy: 0.9649122807017544\n",
      "Random Forest with 100 trees - Accuracy: 0.9649122807017544\n",
      "Random Forest with 200 trees - Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "\n",
    "for n in n_estimators_list:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Random Forest with {n} trees - Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\justo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with Logistic Regression - AUC Score: 0.99737962659679\n"
     ]
    }
   ],
   "source": [
    "#28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train a Bagging Classifier with Logistic Regression\n",
    "bagging_lr = BaggingClassifier(estimator=LogisticRegression(max_iter=500), n_estimators=50, random_state=42)\n",
    "bagging_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = bagging_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"Bagging Classifier with Logistic Regression - AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "Feature 0: 0.0014\n",
      "Feature 1: 0.0211\n",
      "Feature 2: 0.0025\n",
      "Feature 3: 0.0051\n",
      "Feature 4: 0.0067\n",
      "Feature 5: 0.0008\n",
      "Feature 6: 0.0047\n",
      "Feature 7: 0.2028\n",
      "Feature 8: 0.0027\n",
      "Feature 9: 0.0021\n",
      "Feature 10: 0.0037\n",
      "Feature 11: 0.0038\n",
      "Feature 12: 0.0039\n",
      "Feature 13: 0.0102\n",
      "Feature 14: 0.0035\n",
      "Feature 15: 0.0017\n",
      "Feature 16: 0.0066\n",
      "Feature 17: 0.0033\n",
      "Feature 18: 0.0035\n",
      "Feature 19: 0.0053\n",
      "Feature 20: 0.1035\n",
      "Feature 21: 0.0255\n",
      "Feature 22: 0.1589\n",
      "Feature 23: 0.1182\n",
      "Feature 24: 0.0085\n",
      "Feature 25: 0.0018\n",
      "Feature 26: 0.0118\n",
      "Feature 27: 0.2699\n",
      "Feature 28: 0.0042\n",
      "Feature 29: 0.0024\n"
     ]
    }
   ],
   "source": [
    "#29. Train a Random Forest Regressor and analyze feature importance scores.\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for i, importance in enumerate(rf_reg.feature_importances_):\n",
    "    print(f\"Feature {i}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model (Bagging + Random Forest) Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "#30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create individual classifiers\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train ensemble model using VotingClassifier\n",
    "ensemble_clf = VotingClassifier(estimators=[('bagging', bagging_clf), ('random_forest', rf_clf)], voting='hard')\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Model (Bagging + Random Forest) Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy: 0.9626373626373625\n"
     ]
    }
   ],
   "source": [
    "#31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor with 10 estimators - MSE: 0.037456140350877194\n",
      "Bagging Regressor with 50 estimators - MSE: 0.03357543859649123\n",
      "Bagging Regressor with 100 estimators - MSE: 0.03204298245614035\n",
      "Bagging Regressor with 200 estimators - MSE: 0.0321390350877193\n"
     ]
    }
   ],
   "source": [
    "#32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Try different numbers of base estimators\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "\n",
    "for n in n_estimators_list:\n",
    "    bagging_reg = BaggingRegressor(n_estimators=n, random_state=42)\n",
    "    bagging_reg.fit(X_train, y_train)\n",
    "    y_pred = bagging_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Bagging Regressor with {n} estimators - MSE: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified Sample Indices: [ 8 20 77 82]\n"
     ]
    }
   ],
   "source": [
    "#33. Train a Random Forest Classifier and analyze misclassified samples.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified_indices = (y_pred != y_test)\n",
    "print(\"Misclassified Sample Indices:\", misclassified_indices.nonzero()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9473684210526315\n",
      "Bagging Classifier Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "#34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Train a Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy_bagging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#35. Train a Random Forest Classifier and visualize the confusion matrix.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svm', SVC(probability=True)),\n",
    "]\n",
    "\n",
    "# Define Stacking Classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = stacking_clf.score(X_test, y_test)\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "worst area: 0.1539\n",
      "worst concave points: 0.1447\n",
      "mean concave points: 0.1062\n",
      "worst radius: 0.0780\n",
      "mean concavity: 0.0680\n"
     ]
    }
   ],
   "source": [
    "#37. Train a Random Forest Classifier and print the top 5 most important features.\n",
    "\n",
    "# Get feature importances\n",
    "import numpy as np\n",
    "\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Print top 5 features\n",
    "print(\"Top 5 Important Features:\")\n",
    "for i in range(5):\n",
    "    print(f\"{data.feature_names[sorted_indices[i]]}: {feature_importances[sorted_indices[i]]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9583, Recall: 0.9718, F1-score: 0.9650\n"
     ]
    }
   ],
   "source": [
    "#38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred_bagging)\n",
    "recall = recall_score(y_test, y_pred_bagging)\n",
    "f1 = f1_score(y_test, y_pred_bagging)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with max_depth=5 - Accuracy: 0.9649122807017544\n",
      "Random Forest with max_depth=10 - Accuracy: 0.9649122807017544\n",
      "Random Forest with max_depth=20 - Accuracy: 0.9649122807017544\n",
      "Random Forest with max_depth=None - Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
    "\n",
    "max_depth_values = [5, 10, 20, None]\n",
    "\n",
    "for depth in max_depth_values:\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    accuracy = rf.score(X_test, y_test)\n",
    "    print(f\"Random Forest with max_depth={depth} - Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor with Decision Tree - MSE: 0.03357543859649123\n",
      "Bagging Regressor with KNeighbors - MSE: 0.02909782456140351\n"
     ]
    }
   ],
   "source": [
    "#40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Using Decision Tree\n",
    "bagging_dt = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
    "bagging_dt.fit(X_train, y_train)\n",
    "mse_dt = mean_squared_error(y_test, bagging_dt.predict(X_test))\n",
    "\n",
    "# Using KNN\n",
    "bagging_knn = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
    "bagging_knn.fit(X_train, y_train)\n",
    "mse_knn = mean_squared_error(y_test, bagging_knn.predict(X_test))\n",
    "\n",
    "print(\"Bagging Regressor with Decision Tree - MSE:\", mse_dt)\n",
    "print(\"Bagging Regressor with KNeighbors - MSE:\", mse_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier ROC-AUC Score: 0.9952505732066819\n"
     ]
    }
   ],
   "source": [
    "#41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "print(\"Random Forest Classifier ROC-AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Cross-Validation Accuracy Scores: [0.9122807  0.92105263 0.98245614 0.95614035 1.        ]\n",
      "Mean Accuracy: 0.9543859649122808\n"
     ]
    }
   ],
   "source": [
    "#42. Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Bagging Classifier Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Compute precision-recall values\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(recall, precision, marker='.', label='Random Forest')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier (RF + Logistic Regression) Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "]\n",
    "\n",
    "# Define Stacking Classifier\n",
    "stacking_clf2 = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
    "stacking_clf2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = stacking_clf2.score(X_test, y_test)\n",
    "print(\"Stacking Classifier (RF + Logistic Regression) Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor with bootstrap=True - MSE: 0.03357543859649123\n",
      "Bagging Regressor with bootstrap=False - MSE: 0.05620350877192982\n"
     ]
    }
   ],
   "source": [
    "#45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
    "\n",
    "# Test different bootstrap sample sizes\n",
    "bootstrap_values = [True, False]\n",
    "\n",
    "for bootstrap in bootstrap_values:\n",
    "    bagging_reg = BaggingRegressor(n_estimators=50, bootstrap=bootstrap, random_state=42)\n",
    "    bagging_reg.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test, bagging_reg.predict(X_test))\n",
    "    print(f\"Bagging Regressor with bootstrap={bootstrap} - MSE: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
